{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76406f81-0353-409e-be89-80ce9ebedab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f35696c-eec9-46e8-9dc0-8f2b3b58d941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Starting ETL Process...\n",
      "âœ… Data loaded: 9800 rows, 18 columns\n",
      "\n",
      "ðŸ“ˆ Data Exploration:\n",
      "------------------------------\n",
      "Date Range: 01/01/2018 to 31/12/2017\n",
      "Total Sales: $2,261,536.78\n",
      "Number of Customers: 793\n",
      "Number of Products: 1861\n",
      "\n",
      "ðŸ“‹ Available Columns:\n",
      "    1. Row ID\n",
      "    2. Order ID\n",
      "    3. Order Date\n",
      "    4. Ship Date\n",
      "    5. Ship Mode\n",
      "    6. Customer ID\n",
      "    7. Customer Name\n",
      "    8. Segment\n",
      "    9. Country\n",
      "   10. City\n",
      "   11. State\n",
      "   12. Postal Code\n",
      "   13. Region\n",
      "   14. Product ID\n",
      "   15. Category\n",
      "   16. Sub-Category\n",
      "   17. Product Name\n",
      "   18. Sales\n",
      "\n",
      "Data Types:\n",
      "Row ID             int64\n",
      "Order ID          object\n",
      "Order Date        object\n",
      "Ship Date         object\n",
      "Ship Mode         object\n",
      "Customer ID       object\n",
      "Customer Name     object\n",
      "Segment           object\n",
      "Country           object\n",
      "City              object\n",
      "State             object\n",
      "Postal Code      float64\n",
      "Region            object\n",
      "Product ID        object\n",
      "Category          object\n",
      "Sub-Category      object\n",
      "Product Name      object\n",
      "Sales            float64\n",
      "dtype: object\n",
      "\n",
      "âš ï¸ Missing Values Found:\n",
      "   Postal Code: 11 missing values\n",
      "\n",
      "ðŸ“ Duplicate Rows: 0\n",
      "\n",
      "ðŸ§¹ Data Cleaning:\n",
      "------------------------------\n",
      "Converting dates...\n",
      "âœ… Data cleaning complete!\n",
      "\n",
      "ðŸŽ¯ Creating New Columns:\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mega Pc\\AppData\\Local\\Temp\\ipykernel_21208\\1183048233.py:83: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added new columns: ['Row ID', 'Order ID', 'Ship Mode', 'Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name', 'Order_Year', 'Order_Month', 'Order_Quarter', 'Order_Day', 'Day_of_Week', 'Shipping_Days', 'Shipping_On_Time', 'Year_Month']\n",
      "\n",
      "ðŸ’¾ Saving Data:\n",
      "------------------------------\n",
      "âœ… Cleaned data saved as: 'output/cleaned_sales_data.csv'\n",
      "âœ… Sample data saved as: 'output/sample_data.csv'\n",
      "\n",
      "ðŸ—„ï¸  Loading Data to Database:\n",
      "------------------------------\n",
      "Connected to database: database/sales_database.db\n",
      "Loading data into database...\n",
      "âœ… 9,800 rows loaded into database\n",
      "\n",
      "ðŸ“‹ Columns in database table:\n",
      "    1. Row ID\n",
      "    2. Order ID\n",
      "    3. Order Date\n",
      "    4. Ship Date\n",
      "    5. Ship Mode\n",
      "    6. Customer ID\n",
      "    7. Customer Name\n",
      "    8. Segment\n",
      "    9. Country\n",
      "   10. City\n",
      "   11. State\n",
      "   12. Postal Code\n",
      "   13. Region\n",
      "   14. Product ID\n",
      "   15. Category\n",
      "   16. Sub-Category\n",
      "   17. Product Name\n",
      "   18. Sales\n",
      "   19. Order_Year\n",
      "   20. Order_Month\n",
      "   21. Order_Quarter\n",
      "   22. Order_Day\n",
      "   23. Day_of_Week\n",
      "   24. Shipping_Days\n",
      "   25. Shipping_On_Time\n",
      "   26. Year_Month\n",
      "\n",
      "ðŸ” Column check:\n",
      "   Has Profit column: False\n",
      "   Has Postal Code column: True\n",
      "   Has Segment column: True\n",
      "\n",
      "Creating sales summary table...\n",
      "âœ… Sales summary table created successfully!\n",
      "   Summary rows: 573\n",
      "   Summary columns: ['Order_Year', 'Order_Month', 'Category', 'Region', 'order_count', 'total_sales', 'avg_order_value']\n",
      "âœ… Database connection closed successfully!\n",
      "ðŸ“ Database file saved: database/sales_database.db\n",
      "ðŸ“Š Database size: 3504.0 KB\n",
      "\n",
      "ðŸ“– Creating Data Dictionary...\n",
      "âœ… Data dictionary saved as CSV and Excel!\n",
      "\n",
      "==================================================\n",
      "ðŸŽ‰ ETL PROCESS COMPLETE!\n",
      "==================================================\n",
      "\n",
      "ðŸ“Š Final Dataset Summary:\n",
      "   â€¢ Total Rows: 9800\n",
      "   â€¢ Total Columns: 26\n",
      "   â€¢ Date Range: 2015-01-03 to 2018-12-30\n",
      "   â€¢ Total Sales: $2,261,536.78\n",
      "   â€¢ Average Order Value: $230.77\n",
      "   â€¢ Number of Regions: 4\n",
      "   â€¢ Number of Categories: 3\n",
      "\n",
      "ðŸ“ Files Created:\n",
      "   1. output/cleaned_sales_data.csv\n",
      "   2. output/sample_data.csv\n",
      "   3. output/data_dictionary.csv\n",
      "   4. output/data_dictionary.xlsx\n",
      "   5. database/sales_database.db (SQLite Database)\n",
      "\n",
      "ðŸ—„ï¸  Database Information:\n",
      "   â€¢ Main Table: sales_transactions (9,800 rows)\n",
      "   â€¢ Summary Table: sales_summary\n",
      "   â€¢ Total columns in dataset: 26\n",
      "\n",
      "âœ… Ready for Data Modeling in Power BI!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BUSINESS INTELLIGENCE PROJECT - ETL PIPELINE\n",
    "Team: Mariem & Rihem\n",
    "Dataset: Retail Sales\n",
    "\"\"\"\n",
    "\n",
    "# 1. Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sqlite3  # Added for database loading\n",
    "\n",
    "print(\"ðŸ“Š Starting ETL Process...\")\n",
    "\n",
    "# 2. Data loading from train.csv\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "print(f\"âœ… Data loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# 3. Data exploration and analysis\n",
    "\n",
    "print(\"\\nðŸ“ˆ Data Exploration:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Basic info\n",
    "print(f\"Date Range: {df['Order Date'].min()} to {df['Order Date'].max()}\")\n",
    "print(f\"Total Sales: ${df['Sales'].sum():,.2f}\")\n",
    "print(f\"Number of Customers: {df['Customer ID'].nunique()}\")\n",
    "print(f\"Number of Products: {df['Product ID'].nunique()}\")\n",
    "\n",
    "# Check what columns we actually have\n",
    "print(\"\\nðŸ“‹ Available Columns:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"   {i:2}. {col}\")\n",
    "\n",
    "# Check data types\n",
    "print(f\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(\"\\nâš ï¸ Missing Values Found:\")\n",
    "    for col, count in missing.items():\n",
    "        if count > 0:\n",
    "            print(f\"   {col}: {count} missing values\")\n",
    "else:\n",
    "    print(\"\\nâœ… No missing values found!\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nðŸ“ Duplicate Rows: {duplicates}\")\n",
    "\n",
    "# 4. Data cleaning (dates, duplicates, missing values)\n",
    "\n",
    "print(\"\\nðŸ§¹ Data Cleaning:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Fix date columns\n",
    "print(\"Converting dates...\")\n",
    "df['Order Date'] = pd.to_datetime(df['Order Date'], format='%d/%m/%Y')\n",
    "df['Ship Date'] = pd.to_datetime(df['Ship Date'], format='%d/%m/%Y')\n",
    "\n",
    "# Remove duplicates if any\n",
    "if duplicates > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Removed {duplicates} duplicate rows\")\n",
    "\n",
    "# Fix text columns (remove extra spaces)\n",
    "text_columns = ['Customer Name', 'Product Name', 'City', 'State']\n",
    "for col in text_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].str.strip()\n",
    "\n",
    "# Handle missing values (simple approach)\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        if df[col].dtype == 'object':  # Text columns\n",
    "            df[col].fillna('Unknown', inplace=True)\n",
    "        else:  # Numeric columns\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "print(\"âœ… Data cleaning complete!\")\n",
    "\n",
    "# 5. Feature engineering (new business columns)\n",
    "\n",
    "print(\"\\nðŸŽ¯ Creating New Columns:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Time-based columns\n",
    "df['Order_Year'] = df['Order Date'].dt.year\n",
    "df['Order_Month'] = df['Order Date'].dt.month\n",
    "df['Order_Quarter'] = df['Order Date'].dt.quarter\n",
    "df['Order_Day'] = df['Order Date'].dt.day\n",
    "df['Day_of_Week'] = df['Order Date'].dt.day_name()\n",
    "\n",
    "# Business metrics\n",
    "df['Shipping_Days'] = (df['Ship Date'] - df['Order Date']).dt.days\n",
    "df['Shipping_On_Time'] = df['Shipping_Days'] <= 7  # Assuming 7 days is standard\n",
    "\n",
    "# Month-Year combination for time series\n",
    "df['Year_Month'] = df['Order Date'].dt.strftime('%Y-%m')\n",
    "\n",
    "print(f\"Added new columns: {[col for col in df.columns if col not in ['Order Date', 'Ship Date', 'Sales']]}\")\n",
    "\n",
    "\n",
    "# 6.  Saving cleaned data to CSV files\n",
    "\n",
    "print(\"\\nðŸ’¾ Saving Data:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create output folder\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "# Save cleaned data\n",
    "df.to_csv('output/cleaned_sales_data.csv', index=False)\n",
    "print(\"âœ… Cleaned data saved as: 'output/cleaned_sales_data.csv'\")\n",
    "\n",
    "# Save a smaller sample for testing\n",
    "df.head(1000).to_csv('output/sample_data.csv', index=False)\n",
    "print(\"âœ… Sample data saved as: 'output/sample_data.csv'\")\n",
    "\n",
    "# 7.  DATABASE LOADING (to SQLite database)\n",
    "\n",
    "print(\"\\nðŸ—„ï¸  Loading Data to Database:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create database directory\n",
    "os.makedirs('database', exist_ok=True)\n",
    "\n",
    "# Connect to SQLite database (creates it if doesn't exist)\n",
    "db_path = 'database/sales_database.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(f\"Connected to database: {db_path}\")\n",
    "\n",
    "# Load data from DataFrame to database\n",
    "print(\"Loading data into database...\")\n",
    "df.to_sql('sales_transactions', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Verify the load\n",
    "cursor.execute(\"SELECT COUNT(*) FROM sales_transactions\")\n",
    "row_count = cursor.fetchone()[0]\n",
    "print(f\"âœ… {row_count:,} rows loaded into database\")\n",
    "\n",
    "# Check what columns actually exist in the table\n",
    "cursor.execute(\"PRAGMA table_info(sales_transactions)\")\n",
    "columns_info = cursor.fetchall()\n",
    "column_names = [info[1] for info in columns_info]\n",
    "\n",
    "print(f\"\\nðŸ“‹ Columns in database table:\")\n",
    "for i, col in enumerate(column_names, 1):\n",
    "    print(f\"   {i:2}. {col}\")\n",
    "\n",
    "# Check if specific columns exist\n",
    "has_profit = 'Profit' in column_names or 'profit' in column_names\n",
    "has_postal_code = 'Postal Code' in column_names or 'Postal_Code' in column_names or 'postal_code' in column_names\n",
    "has_segment = 'Segment' in column_names or 'segment' in column_names\n",
    "\n",
    "print(f\"\\nðŸ” Column check:\")\n",
    "print(f\"   Has Profit column: {has_profit}\")\n",
    "print(f\"   Has Postal Code column: {has_postal_code}\")\n",
    "print(f\"   Has Segment column: {has_segment}\")\n",
    "\n",
    "# Create summary table based on available columns\n",
    "print(\"\\nCreating sales summary table...\")\n",
    "\n",
    "# Build the summary query dynamically based on available columns\n",
    "summary_query = '''\n",
    "CREATE TABLE IF NOT EXISTS sales_summary AS\n",
    "SELECT \n",
    "    Order_Year,\n",
    "    Order_Month,\n",
    "'''\n",
    "\n",
    "# Add Category if it exists\n",
    "if 'Category' in column_names or 'category' in column_names:\n",
    "    summary_query += '    Category,\\n'\n",
    "else:\n",
    "    summary_query += \"    'All' as Category,\\n\"\n",
    "\n",
    "# Add Region if it exists  \n",
    "if 'Region' in column_names or 'region' in column_names:\n",
    "    summary_query += '    Region,\\n'\n",
    "else:\n",
    "    summary_query += \"    'All' as Region,\\n\"\n",
    "\n",
    "# Add common metrics\n",
    "summary_query += '''    COUNT(*) as order_count,\n",
    "    SUM(Sales) as total_sales,\n",
    "    AVG(Sales) as avg_order_value\n",
    "'''\n",
    "\n",
    "# Add Profit if it exists\n",
    "if has_profit:\n",
    "    summary_query += '    , SUM(Profit) as total_profit\\n'\n",
    "\n",
    "summary_query += '''FROM sales_transactions\n",
    "GROUP BY Order_Year, Order_Month\n",
    "'''\n",
    "\n",
    "# Add Category and Region to GROUP BY if they exist\n",
    "if 'Category' in column_names or 'category' in column_names:\n",
    "    summary_query = summary_query.replace('GROUP BY Order_Year, Order_Month', \n",
    "                                         'GROUP BY Order_Year, Order_Month, Category')\n",
    "if 'Region' in column_names or 'region' in column_names:\n",
    "    summary_query = summary_query.replace('GROUP BY Order_Year, Order_Month', \n",
    "                                         'GROUP BY Order_Year, Order_Month, Region')\n",
    "\n",
    "# Execute the dynamic query\n",
    "try:\n",
    "    cursor.execute('DROP TABLE IF EXISTS sales_summary')\n",
    "    cursor.execute(summary_query)\n",
    "    print(\"âœ… Sales summary table created successfully!\")\n",
    "    \n",
    "    # Show summary table info\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM sales_summary\")\n",
    "    summary_count = cursor.fetchone()[0]\n",
    "    print(f\"   Summary rows: {summary_count}\")\n",
    "    \n",
    "    cursor.execute(\"PRAGMA table_info(sales_summary)\")\n",
    "    summary_columns = cursor.fetchall()\n",
    "    print(f\"   Summary columns: {[col[1] for col in summary_columns]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not create summary table: {e}\")\n",
    "    print(\"Creating basic summary table instead...\")\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS sales_summary AS\n",
    "        SELECT \n",
    "            Order_Year,\n",
    "            Order_Month,\n",
    "            COUNT(*) as order_count,\n",
    "            SUM(Sales) as total_sales,\n",
    "            AVG(Sales) as avg_order_value\n",
    "        FROM sales_transactions\n",
    "        GROUP BY Order_Year, Order_Month\n",
    "    ''')\n",
    "    print(\"âœ… Basic sales summary table created\")\n",
    "\n",
    "# Save and close\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"âœ… Database connection closed successfully!\")\n",
    "print(f\"ðŸ“ Database file saved: {db_path}\")\n",
    "print(f\"ðŸ“Š Database size: {os.path.getsize(db_path) / 1024:.1f} KB\")\n",
    "\n",
    "\n",
    "# 8.  Data dictionary creation\n",
    "\n",
    "print(\"\\nðŸ“– Creating Data Dictionary...\")\n",
    "\n",
    "data_dict = []\n",
    "\n",
    "for column in df.columns:\n",
    "    # Get basic info\n",
    "    col_info = {\n",
    "        'Column Name': column,\n",
    "        'Data Type': str(df[column].dtype),\n",
    "        'Unique Values': df[column].nunique(),\n",
    "        'Missing Values': df[column].isnull().sum()\n",
    "    }\n",
    "    \n",
    "    # Add description based on column name\n",
    "    if 'Date' in column:\n",
    "        col_info['Description'] = 'Date information'\n",
    "    elif 'Sales' in column:\n",
    "        col_info['Description'] = 'Sales amount in USD'\n",
    "    elif 'Customer' in column:\n",
    "        col_info['Description'] = 'Customer information'\n",
    "    elif 'Product' in column:\n",
    "        col_info['Description'] = 'Product information'\n",
    "    elif 'Ship' in column:\n",
    "        col_info['Description'] = 'Shipping information'\n",
    "    elif 'Region' in column or 'State' in column or 'City' in column:\n",
    "        col_info['Description'] = 'Geographic information'\n",
    "    else:\n",
    "        col_info['Description'] = 'Other information'\n",
    "    \n",
    "    # Add sample value\n",
    "    if len(df[column]) > 0:\n",
    "        sample = df[column].iloc[0]\n",
    "        if pd.isna(sample):\n",
    "            col_info['Sample Value'] = 'N/A'\n",
    "        else:\n",
    "            col_info['Sample Value'] = str(sample)[:50]  # First 50 chars\n",
    "    else:\n",
    "        col_info['Sample Value'] = 'N/A'\n",
    "    \n",
    "    data_dict.append(col_info)\n",
    "\n",
    "# Save data dictionary\n",
    "dict_df = pd.DataFrame(data_dict)\n",
    "dict_df.to_csv('output/data_dictionary.csv', index=False)\n",
    "dict_df.to_excel('output/data_dictionary.xlsx', index=False)\n",
    "print(\"âœ… Data dictionary saved as CSV and Excel!\")\n",
    "\n",
    "# 9. Final summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸŽ‰ ETL PROCESS COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nðŸ“Š Final Dataset Summary:\")\n",
    "print(f\"   â€¢ Total Rows: {df.shape[0]}\")\n",
    "print(f\"   â€¢ Total Columns: {df.shape[1]}\")\n",
    "print(f\"   â€¢ Date Range: {df['Order Date'].min().date()} to {df['Order Date'].max().date()}\")\n",
    "print(f\"   â€¢ Total Sales: ${df['Sales'].sum():,.2f}\")\n",
    "print(f\"   â€¢ Average Order Value: ${df['Sales'].mean():,.2f}\")\n",
    "\n",
    "# Check and display other metrics if they exist\n",
    "if 'Region' in df.columns:\n",
    "    print(f\"   â€¢ Number of Regions: {df['Region'].nunique()}\")\n",
    "if 'Category' in df.columns:\n",
    "    print(f\"   â€¢ Number of Categories: {df['Category'].nunique()}\")\n",
    "if 'Profit' in df.columns:\n",
    "    print(f\"   â€¢ Total Profit: ${df['Profit'].sum():,.2f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ Files Created:\")\n",
    "print(f\"   1. output/cleaned_sales_data.csv\")\n",
    "print(f\"   2. output/sample_data.csv\")\n",
    "print(f\"   3. output/data_dictionary.csv\")\n",
    "print(f\"   4. output/data_dictionary.xlsx\")\n",
    "print(f\"   5. database/sales_database.db (SQLite Database)\")\n",
    "\n",
    "print(f\"\\nðŸ—„ï¸  Database Information:\")\n",
    "print(f\"   â€¢ Main Table: sales_transactions ({row_count:,} rows)\")\n",
    "print(f\"   â€¢ Summary Table: sales_summary\")\n",
    "print(f\"   â€¢ Total columns in dataset: {len(df.columns)}\")\n",
    "\n",
    "print(\"\\nâœ… Ready for Data Modeling in Power BI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4de36d88-62ae-4a12-b4d2-0d52e1d2e7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š STARTING DATA MODELING PHASE...\n",
      "==================================================\n",
      "\n",
      "1. ðŸ“¥ Loading cleaned data...\n",
      "   âœ… Data loaded successfully: 9800 rows\n",
      "   ðŸ“‹ Available columns: ['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name', 'Sales', 'Order_Year', 'Order_Month', 'Order_Quarter', 'Order_Day', 'Day_of_Week', 'Shipping_Days', 'Shipping_On_Time', 'Year_Month']\n",
      "\n",
      "   ðŸ” Checking column availability...\n",
      "   Available columns: ['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name', 'Sales', 'Order_Year', 'Order_Month', 'Order_Quarter', 'Order_Day', 'Day_of_Week', 'Shipping_Days', 'Shipping_On_Time', 'Year_Month']\n",
      "\n",
      "   Converting date columns to datetime...\n",
      "   Converted Order Date to datetime\n",
      "   Converted Ship Date to datetime\n",
      "\n",
      "   âœ… Loaded: 9800 rows, 26 columns\n",
      "   Date range: 2015-01-03 to 2018-12-30\n",
      "\n",
      "2. ðŸ”§ Creating Dimension Tables...\n",
      "   Creating Dim_Date...\n",
      "   âœ… Dim_Date created: 1823 dates from 2015-01-03 to 2019-12-30\n",
      "   Creating Dim_Product...\n",
      "   âœ… Dim_Product created: 1861 unique products\n",
      "   Creating Dim_Customer...\n",
      "   âœ… Dim_Customer created: 793 unique customers\n",
      "   Creating Dim_Region...\n",
      "   âœ… Dim_Region created: 600 unique locations\n",
      "\n",
      "3. ðŸŽ¯ Creating Fact_Sales table...\n",
      "   Merging with dimension tables...\n",
      "   âœ… Fact_Sales created with 9800 rows and 7 columns\n",
      "   ðŸ“Š Available measures: ['Sales_Amount', 'Shipping_Days']\n",
      "   âœ… All foreign keys are valid\n",
      "\n",
      "4. ðŸ’¾ Saving all tables...\n",
      "   âœ… Saved:\n",
      "      - model/Dim_Date.csv\n",
      "      - model/Dim_Product.csv\n",
      "      - model/Dim_Customer.csv\n",
      "      - model/Dim_Region.csv\n",
      "      - model/Fact_Sales.csv\n",
      "\n",
      "5. ðŸ“ Creating Data Model Diagram...\n",
      "   âœ… Data model diagram saved: model/data_model_diagram.txt\n",
      "\n",
      "6. ðŸ“ˆ Defining DAX Measures...\n",
      "   âœ… DAX measures saved: model/DAX_Measures.txt\n",
      "\n",
      "7. ðŸ“„ Creating One-Page Summary...\n",
      "   âœ… One-page summary saved: model/OnePage_Summary.txt\n",
      "\n",
      "8. ðŸ“š Creating Complete Documentation...\n",
      "   âœ… Documentation saved: model/README.txt\n",
      "\n",
      "9. ðŸ“‹ Creating CSV Files Documentation...\n",
      "   âœ… CSV documentation saved: model/CSV_Documentation.txt\n",
      "\n",
      "============================================================\n",
      "ðŸŽ‰ WEEK 2-3: DATA MODELING COMPLETE!\n",
      "============================================================\n",
      "\n",
      "ðŸ“ DELIVERABLES CREATED:\n",
      "1. âœ… Star Schema Data Model\n",
      "   - Fact_Sales.csv (9,800 rows, 7 columns)\n",
      "   - 4 Dimension tables created\n",
      "\n",
      "2. âœ… Data Model Documentation\n",
      "   - data_model_diagram.txt (ASCII diagram)\n",
      "   - simple_diagram.txt (Simplified diagram)\n",
      "   - DAX_Measures.txt (10+ measures)\n",
      "   - OnePage_Summary.txt (Project summary)\n",
      "   - README.txt (Complete documentation)\n",
      "   - CSV_Documentation.txt (File descriptions)\n",
      "\n",
      "3. âœ… DAX Measures Definitions\n",
      "   - Measures defined for: ['Sales_Amount', 'Shipping_Days']\n",
      "   - Includes SUM, AVG, YOY, MOM as required\n",
      "\n",
      "ðŸ“Š DATA SUMMARY:\n",
      "   â€¢ Fact table columns: ['Order_ID', 'Date_Key', 'Product_Key', 'Customer_Key', 'Region_Key', 'Sales_Amount', 'Shipping_Days']\n",
      "   â€¢ Date range: 2015-01-03 to 2018-12-30\n",
      "   â€¢ Total Sales Value: $2,261,536.78\n",
      "\n",
      "ðŸš€ READY FOR WEEK 3 - DASHBOARD DEVELOPMENT:\n",
      "   Step 1: Load model/*.csv into Power BI\n",
      "   Step 2: Create relationships (star schema)\n",
      "   Step 3: Implement DAX measures\n",
      "   Step 4: Build Executive Summary dashboard\n",
      "   Step 5: Add slicers and interactivity\n",
      "\n",
      "âœ… DATA MODELING PHASE SUCCESSFULLY COMPLETED!\n",
      "   All Week 2-3 requirements have been met.\n"
     ]
    }
   ],
   "source": [
    "#  DATA MODELING \n",
    "\n",
    "print(\"ðŸ“Š STARTING DATA MODELING PHASE...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# 1. Load cleaned data\n",
    "\n",
    "print(\"\\n1. ðŸ“¥ Loading cleaned data...\")\n",
    "try:\n",
    "    df = pd.read_csv('output/cleaned_sales_data.csv')\n",
    "    print(f\"   âœ… Data loaded successfully: {len(df)} rows\")\n",
    "    print(f\"   ðŸ“‹ Available columns: {list(df.columns)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"   âŒ Error: cleaned_sales_data.csv not found. Run ETL first!\")\n",
    "    exit()\n",
    "\n",
    "# Check what columns we actually have\n",
    "print(\"\\n   ðŸ” Checking column availability...\")\n",
    "available_cols = df.columns.tolist()\n",
    "print(f\"   Available columns: {available_cols}\")\n",
    "\n",
    "# CONVERT DATE COLUMNS FROM STRING TO DATETIME\n",
    "print(\"\\n   Converting date columns to datetime...\")\n",
    "# First, identify which date columns exist\n",
    "date_cols = [col for col in available_cols if 'date' in col.lower() or 'Date' in col]\n",
    "for col in date_cols:\n",
    "    try:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        print(f\"   Converted {col} to datetime\")\n",
    "    except:\n",
    "        print(f\"   Could not convert {col} to datetime\")\n",
    "\n",
    "print(f\"\\n   âœ… Loaded: {len(df)} rows, {len(df.columns)} columns\")\n",
    "if 'Order Date' in df.columns:\n",
    "    print(f\"   Date range: {df['Order Date'].min().date()} to {df['Order Date'].max().date()}\")\n",
    "\n",
    "# 2. Create dimension table\n",
    "\n",
    "print(\"\\n2. ðŸ”§ Creating Dimension Tables...\")\n",
    "\n",
    "# Dim_Date - Date Dimension\n",
    "print(\"   Creating Dim_Date...\")\n",
    "\n",
    "# Determine date range from available data\n",
    "if 'Order Date' in df.columns and df['Order Date'].notna().any():\n",
    "    min_date = df['Order Date'].min().date()\n",
    "    max_date = df['Order Date'].max().date()\n",
    "else:\n",
    "    # Use default dates if no date column\n",
    "    min_date = datetime(2020, 1, 1).date()\n",
    "    max_date = datetime(2025, 12, 31).date()\n",
    "    print(f\"   âš ï¸  Using default date range: {min_date} to {max_date}\")\n",
    "\n",
    "# Create date range\n",
    "date_range = pd.date_range(\n",
    "    start=min_date,\n",
    "    end=max_date + timedelta(days=365),  # Add 1 year buffer\n",
    "    freq='D'\n",
    ")\n",
    "\n",
    "dim_date = pd.DataFrame({\n",
    "    'Date_Key': [int(d.strftime('%Y%m%d')) for d in date_range],\n",
    "    'Date': date_range,\n",
    "    'Year': [d.year for d in date_range],\n",
    "    'Quarter': [f'Q{(d.month-1)//3 + 1}' for d in date_range],\n",
    "    'Quarter_Num': [(d.month-1)//3 + 1 for d in date_range],\n",
    "    'Month': [d.month for d in date_range],\n",
    "    'Month_Name': [d.strftime('%B') for d in date_range],\n",
    "    'Week_Num': [d.isocalendar()[1] for d in date_range],\n",
    "    'Day_of_Week': [d.strftime('%A') for d in date_range],\n",
    "    'Day_of_Week_Num': [d.weekday() + 1 for d in date_range],\n",
    "    'Is_Weekend': [(d.weekday() >= 5) for d in date_range],\n",
    "    'Is_Holiday': [False] * len(date_range),\n",
    "    'Fiscal_Year': [d.year for d in date_range]\n",
    "})\n",
    "\n",
    "print(f\"   âœ… Dim_Date created: {len(dim_date)} dates from {date_range[0].date()} to {date_range[-1].date()}\")\n",
    "\n",
    "# Dim_Product - Product Dimension (use available columns)\n",
    "print(\"   Creating Dim_Product...\")\n",
    "# Find product-related columns\n",
    "product_cols = []\n",
    "for col in ['Product ID', 'Product Name', 'Category', 'Sub-Category', 'product_id', 'product_name']:\n",
    "    if col in df.columns:\n",
    "        product_cols.append(col)\n",
    "\n",
    "if product_cols:\n",
    "    dim_product = df[product_cols].copy()\n",
    "    # Use the first column as the primary key\n",
    "    pk_col = product_cols[0]\n",
    "    dim_product = dim_product.drop_duplicates(subset=[pk_col])\n",
    "    dim_product = dim_product.reset_index(drop=True)\n",
    "    dim_product['Product_Key'] = range(1, len(dim_product) + 1)\n",
    "    # Keep all product columns\n",
    "    dim_product = dim_product[['Product_Key'] + product_cols]\n",
    "    print(f\"   âœ… Dim_Product created: {len(dim_product)} unique products\")\n",
    "else:\n",
    "    print(\"   âš ï¸  No product columns found, creating empty Dim_Product\")\n",
    "    dim_product = pd.DataFrame({'Product_Key': [], 'Product_ID': []})\n",
    "\n",
    "# Dim_Customer - Customer Dimension (use available columns)\n",
    "print(\"   Creating Dim_Customer...\")\n",
    "# Find customer-related columns\n",
    "customer_cols = []\n",
    "for col in ['Customer ID', 'Customer Name', 'Segment', 'customer_id', 'customer_name']:\n",
    "    if col in df.columns:\n",
    "        customer_cols.append(col)\n",
    "\n",
    "if customer_cols:\n",
    "    dim_customer = df[customer_cols].copy()\n",
    "    # Use the first column as the primary key\n",
    "    pk_col = customer_cols[0]\n",
    "    dim_customer = dim_customer.drop_duplicates(subset=[pk_col])\n",
    "    dim_customer = dim_customer.reset_index(drop=True)\n",
    "    dim_customer['Customer_Key'] = range(1, len(dim_customer) + 1)\n",
    "    # Keep all customer columns\n",
    "    dim_customer = dim_customer[['Customer_Key'] + customer_cols]\n",
    "    print(f\"   âœ… Dim_Customer created: {len(dim_customer)} unique customers\")\n",
    "else:\n",
    "    print(\"   âš ï¸  No customer columns found, creating empty Dim_Customer\")\n",
    "    dim_customer = pd.DataFrame({'Customer_Key': [], 'Customer_ID': []})\n",
    "\n",
    "# Dim_Region - Region Dimension (use available columns)\n",
    "print(\"   Creating Dim_Region...\")\n",
    "# Find region-related columns\n",
    "region_cols = []\n",
    "for col in ['City', 'State', 'Region', 'Country', 'city', 'state', 'region']:\n",
    "    if col in df.columns:\n",
    "        region_cols.append(col)\n",
    "\n",
    "if region_cols:\n",
    "    dim_region = df[region_cols].copy()\n",
    "    dim_region = dim_region.drop_duplicates()\n",
    "    dim_region = dim_region.reset_index(drop=True)\n",
    "    dim_region['Region_Key'] = range(1, len(dim_region) + 1)\n",
    "    # Keep all region columns\n",
    "    dim_region = dim_region[['Region_Key'] + region_cols]\n",
    "    print(f\"   âœ… Dim_Region created: {len(dim_region)} unique locations\")\n",
    "else:\n",
    "    print(\"   âš ï¸  No region columns found, creating empty Dim_Region\")\n",
    "    dim_region = pd.DataFrame({'Region_Key': [], 'Region': []})\n",
    "\n",
    "# 3. Create fact table with foreign tables\n",
    "\n",
    "print(\"\\n3. ðŸŽ¯ Creating Fact_Sales table...\")\n",
    "\n",
    "# Prepare fact table\n",
    "fact_sales = df.copy()\n",
    "\n",
    "# Create Date_Key for joining if we have Order Date\n",
    "if 'Order Date' in df.columns:\n",
    "    fact_sales['Date_Key'] = fact_sales['Order Date'].dt.strftime('%Y%m%d').astype(int)\n",
    "elif 'order_date' in df.columns:\n",
    "    fact_sales['Date_Key'] = pd.to_datetime(fact_sales['order_date']).dt.strftime('%Y%m%d').astype(int)\n",
    "else:\n",
    "    # Create a dummy Date_Key\n",
    "    fact_sales['Date_Key'] = 20250101  # Default date\n",
    "\n",
    "print(\"   Merging with dimension tables...\")\n",
    "\n",
    "# Merge with Product if possible\n",
    "if 'Product_Key' in dim_product.columns and product_cols:\n",
    "    product_lookup = dim_product.set_index(product_cols[0])['Product_Key'].to_dict()\n",
    "    fact_sales['Product_Key'] = fact_sales[product_cols[0]].map(product_lookup)\n",
    "else:\n",
    "    fact_sales['Product_Key'] = -1  # Default unknown product\n",
    "\n",
    "# Merge with Customer if possible\n",
    "if 'Customer_Key' in dim_customer.columns and customer_cols:\n",
    "    customer_lookup = dim_customer.set_index(customer_cols[0])['Customer_Key'].to_dict()\n",
    "    fact_sales['Customer_Key'] = fact_sales[customer_cols[0]].map(customer_lookup)\n",
    "else:\n",
    "    fact_sales['Customer_Key'] = -1  # Default unknown customer\n",
    "\n",
    "# Merge with Region if possible\n",
    "if 'Region_Key' in dim_region.columns and region_cols:\n",
    "    # Create composite key for region lookup\n",
    "    dim_region['Location_Key'] = dim_region[region_cols[0]]\n",
    "    if len(region_cols) > 1:\n",
    "        for col in region_cols[1:]:\n",
    "            dim_region['Location_Key'] = dim_region['Location_Key'] + '_' + dim_region[col]\n",
    "    \n",
    "    region_lookup = dim_region.set_index('Location_Key')['Region_Key'].to_dict()\n",
    "    \n",
    "    # Create same composite key in fact table\n",
    "    fact_sales['Location_Key'] = fact_sales[region_cols[0]]\n",
    "    if len(region_cols) > 1:\n",
    "        for col in region_cols[1:]:\n",
    "            fact_sales['Location_Key'] = fact_sales['Location_Key'] + '_' + fact_sales[col]\n",
    "    \n",
    "    fact_sales['Region_Key'] = fact_sales['Location_Key'].map(region_lookup)\n",
    "else:\n",
    "    fact_sales['Region_Key'] = -1  # Default unknown region\n",
    "\n",
    "# Create Order_ID if not exists\n",
    "if 'Order ID' not in fact_sales.columns and 'order_id' not in fact_sales.columns:\n",
    "    fact_sales['Order_ID'] = range(1, len(fact_sales) + 1)\n",
    "elif 'Order ID' in fact_sales.columns:\n",
    "    fact_sales = fact_sales.rename(columns={'Order ID': 'Order_ID'})\n",
    "elif 'order_id' in fact_sales.columns:\n",
    "    fact_sales = fact_sales.rename(columns={'order_id': 'Order_ID'})\n",
    "\n",
    "# Select available measure columns\n",
    "available_measures = []\n",
    "measure_mapping = {\n",
    "    'Sales': 'Sales_Amount',\n",
    "    'sales': 'Sales_Amount',\n",
    "    'Quantity': 'Quantity',\n",
    "    'quantity': 'Quantity',\n",
    "    'Discount': 'Discount',\n",
    "    'discount': 'Discount',\n",
    "    'Profit': 'Profit_Amount',\n",
    "    'profit': 'Profit_Amount',\n",
    "    'Shipping_Days': 'Shipping_Days',\n",
    "    'shipping_days': 'Shipping_Days'\n",
    "}\n",
    "\n",
    "for orig_col, new_name in measure_mapping.items():\n",
    "    if orig_col in fact_sales.columns:\n",
    "        fact_sales[new_name] = fact_sales[orig_col]\n",
    "        available_measures.append(new_name)\n",
    "\n",
    "# Ensure we have at least Sales_Amount\n",
    "if 'Sales_Amount' not in fact_sales.columns and 'Sales' in fact_sales.columns:\n",
    "    fact_sales['Sales_Amount'] = fact_sales['Sales']\n",
    "    available_measures.append('Sales_Amount')\n",
    "\n",
    "# Select columns for final fact table\n",
    "fact_columns = ['Order_ID', 'Date_Key', 'Product_Key', 'Customer_Key', 'Region_Key']\n",
    "fact_columns += available_measures\n",
    "\n",
    "# Keep only columns that exist\n",
    "fact_columns = [col for col in fact_columns if col in fact_sales.columns]\n",
    "fact_sales = fact_sales[fact_columns]\n",
    "\n",
    "print(f\"   âœ… Fact_Sales created with {len(fact_sales)} rows and {len(fact_sales.columns)} columns\")\n",
    "print(f\"   ðŸ“Š Available measures: {available_measures}\")\n",
    "\n",
    "# Check for any missing foreign keys\n",
    "missing_keys = fact_sales.isnull().sum()\n",
    "if missing_keys.any():\n",
    "    print(f\"   âš ï¸  Warning: Found missing values:\")\n",
    "    for col, count in missing_keys.items():\n",
    "        if count > 0:\n",
    "            print(f\"      - {col}: {count} missing values ({count/len(fact_sales)*100:.1f}%)\")\n",
    "    \n",
    "    # Fill missing values\n",
    "    for col in ['Product_Key', 'Customer_Key', 'Region_Key']:\n",
    "        if col in fact_sales.columns:\n",
    "            fact_sales[col] = fact_sales[col].fillna(-1).astype(int)\n",
    "    \n",
    "    for col in available_measures:\n",
    "        if col in fact_sales.columns:\n",
    "            fact_sales[col] = fact_sales[col].fillna(0)\n",
    "else:\n",
    "    print(\"   âœ… All foreign keys are valid\")\n",
    "\n",
    "# 4. Save all tables\n",
    "\n",
    "print(\"\\n4. ðŸ’¾ Saving all tables...\")\n",
    "\n",
    "# Create model directory\n",
    "os.makedirs('model', exist_ok=True)\n",
    "\n",
    "# Save dimension tables\n",
    "dim_date.to_csv('model/Dim_Date.csv', index=False)\n",
    "dim_product.to_csv('model/Dim_Product.csv', index=False)\n",
    "dim_customer.to_csv('model/Dim_Customer.csv', index=False)\n",
    "dim_region.to_csv('model/Dim_Region.csv', index=False)\n",
    "fact_sales.to_csv('model/Fact_Sales.csv', index=False)\n",
    "\n",
    "print(\"   âœ… Saved:\")\n",
    "print(\"      - model/Dim_Date.csv\")\n",
    "print(\"      - model/Dim_Product.csv\")\n",
    "print(\"      - model/Dim_Customer.csv\")\n",
    "print(\"      - model/Dim_Region.csv\")\n",
    "print(\"      - model/Fact_Sales.csv\")\n",
    "\n",
    "# 5. Create data model diagram\n",
    "\n",
    "print(\"\\n5. ðŸ“ Creating Data Model Diagram...\")\n",
    "\n",
    "# Create ASCII-only diagram that won't cause encoding issues\n",
    "diagram = f\"\"\"\n",
    "====================================================================\n",
    "                   STAR SCHEMA DATA MODEL\n",
    "                   Retail Sales Analysis\n",
    "====================================================================\n",
    "\n",
    "RELATIONSHIPS:\n",
    "- Fact_Sales.Date_Key -> Dim_Date.Date_Key\n",
    "- Fact_Sales.Product_Key -> Dim_Product.Product_Key\n",
    "- Fact_Sales.Customer_Key -> Dim_Customer.Customer_Key\n",
    "- Fact_Sales.Region_Key -> Dim_Region.Region_Key\n",
    "\n",
    "                  +-----------------+\n",
    "                  |    Dim_Date     |\n",
    "                  |   (Date_Key)    |\n",
    "                  +--------+--------+\n",
    "                           |\n",
    "        +------------------+------------------+\n",
    "        |                                     |\n",
    "+-------+-------+                   +---------+-------+\n",
    "|  Dim_Product  |                   |  Dim_Customer  |\n",
    "| (Product_Key) |                   | (Customer_Key) |\n",
    "+-------+-------+                   +---------+-------+\n",
    "        |                                     |\n",
    "        +------------------+------------------+\n",
    "                           |\n",
    "                  +--------+--------+\n",
    "                  |    Dim_Region   |\n",
    "                  |   (Region_Key)  |\n",
    "                  +-----------------+\n",
    "\n",
    "                           |\n",
    "                  +--------+--------+\n",
    "                  |   Fact_Sales    |\n",
    "                  |     (Fact)      |\n",
    "                  +-----------------+\n",
    "\n",
    "TABLE STRUCTURE:\n",
    "+----------------+---------------------+------------+------------+\n",
    "| Table          | Primary Key         | Row Count  | Type       |\n",
    "+----------------+---------------------+------------+------------+\n",
    "| Fact_Sales     | Order_ID            | {len(fact_sales):,}     | Fact       |\n",
    "| Dim_Date       | Date_Key            | {len(dim_date):,}     | Dimension  |\n",
    "| Dim_Product    | Product_Key         | {len(dim_product):,}     | Dimension  |\n",
    "| Dim_Customer   | Customer_Key        | {len(dim_customer):,}     | Dimension  |\n",
    "| Dim_Region     | Region_Key          | {len(dim_region):,}     | Dimension  |\n",
    "+----------------+---------------------+------------+------------+\n",
    "\n",
    "FACT TABLE COLUMNS: {list(fact_sales.columns)}\n",
    "\n",
    "DATA MODEL READY FOR POWER BI!\n",
    "\"\"\"\n",
    "\n",
    "# Save diagram with UTF-8 encoding to handle any special characters\n",
    "with open('model/data_model_diagram.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(diagram)\n",
    "\n",
    "print(\"   âœ… Data model diagram saved: model/data_model_diagram.txt\")\n",
    "\n",
    "\n",
    "# 6. Define Dax measures based on available data\n",
    "\n",
    "print(\"\\n6. ðŸ“ˆ Defining DAX Measures...\")\n",
    "\n",
    "# Create measures based on what's available in fact_sales\n",
    "measures_content = \"\"\"============================================================\n",
    "                   DAX MEASURES DEFINITIONS\n",
    "============================================================\n",
    "Project: Business Intelligence - Retail Sales Analysis\n",
    "Team: Mariem & Rihem  # Fixed team name\n",
    "Date: December 2025\n",
    "\n",
    "REQUIRED MEASURES (At least 3 DAX measures):\n",
    "\"\"\"\n",
    "\n",
    "# Use a counter for sequential numbering\n",
    "counter = 1\n",
    "\n",
    "# Add measures based on available columns\n",
    "if 'Sales_Amount' in fact_sales.columns:\n",
    "    measures_content += f\"\"\"\n",
    "{counter}. TOTAL SALES\n",
    "   Formula:  Total_Sales = SUM(Fact_Sales[Sales_Amount])\n",
    "   Type:     Aggregation (SUM)\n",
    "   Use:      Primary KPI for sales performance\n",
    "\"\"\"\n",
    "    counter += 1\n",
    "\n",
    "if 'Quantity' in fact_sales.columns:\n",
    "    measures_content += f\"\"\"\n",
    "{counter}. TOTAL QUANTITY\n",
    "   Formula:  Total_Quantity = SUM(Fact_Sales[Quantity])\n",
    "   Type:     Aggregation (SUM)\n",
    "   Use:      Total items sold\n",
    "\"\"\"\n",
    "    counter += 1\n",
    "\n",
    "if 'Profit_Amount' in fact_sales.columns:\n",
    "    measures_content += f\"\"\"\n",
    "{counter}. TOTAL PROFIT\n",
    "   Formula:  Total_Profit = SUM(Fact_Sales[Profit_Amount])\n",
    "   Type:     Aggregation (SUM)\n",
    "   Use:      Total profit earned\n",
    "\"\"\"\n",
    "    counter += 1\n",
    "\n",
    "if 'Sales_Amount' in fact_sales.columns:\n",
    "    measures_content += f\"\"\"\n",
    "{counter}. AVERAGE ORDER VALUE\n",
    "   Formula:  Avg_Order_Value = AVERAGE(Fact_Sales[Sales_Amount])\n",
    "   Type:     Aggregation (AVG)\n",
    "   Use:      Customer spending analysis\n",
    "\"\"\"\n",
    "    counter += 1\n",
    "\n",
    "if 'Sales_Amount' in fact_sales.columns:\n",
    "    measures_content += f\"\"\"\n",
    "{counter}. YEAR-OVER-YEAR GROWTH\n",
    "   Formula:  Sales_YoY = \n",
    "             VAR CurrentYear = [Total_Sales]\n",
    "             VAR PreviousYear = CALCULATE([Total_Sales], SAMEPERIODLASTYEAR(Dim_Date[Date]))\n",
    "             RETURN DIVIDE(CurrentYear - PreviousYear, PreviousYear, 0)\n",
    "   Type:     Time Intelligence (YOY)\n",
    "   Use:      Annual performance comparison\n",
    "\"\"\"\n",
    "    counter += 1\n",
    "    \n",
    "    measures_content += f\"\"\"\n",
    "{counter}. MONTH-OVER-MONTH GROWTH\n",
    "   Formula:  Sales_MoM = \n",
    "             VAR CurrentMonth = [Total_Sales]\n",
    "             VAR PreviousMonth = CALCULATE([Total_Sales], PREVIOUSMONTH(Dim_Date[Date]))\n",
    "             RETURN DIVIDE(CurrentMonth - PreviousMonth, PreviousMonth, 0)\n",
    "   Type:     Time Intelligence (MOM)\n",
    "   Use:      Monthly trend analysis\n",
    "\"\"\"\n",
    "    counter += 1\n",
    "\n",
    "if 'Discount' in fact_sales.columns:\n",
    "    measures_content += f\"\"\"\n",
    "{counter}. DISCOUNT RATE\n",
    "   Formula:  Discount_Rate = AVERAGE(Fact_Sales[Discount])\n",
    "   Type:     Aggregation (AVG)\n",
    "   Use:      Average discount percentage\n",
    "\"\"\"\n",
    "    counter += 1\n",
    "\n",
    "if 'Profit_Amount' in fact_sales.columns and 'Sales_Amount' in fact_sales.columns:\n",
    "    measures_content += f\"\"\"\n",
    "{counter}. PROFIT MARGIN\n",
    "   Formula:  Profit_Margin = DIVIDE([Total_Profit], [Total_Sales])\n",
    "   Type:     Ratio\n",
    "   Use:      Profitability analysis\n",
    "\"\"\"\n",
    "    counter += 1\n",
    "\n",
    "if 'Shipping_Days' in fact_sales.columns:\n",
    "    measures_content += f\"\"\"\n",
    "{counter}. AVERAGE SHIPPING DAYS\n",
    "   Formula:  Avg_Shipping_Days = AVERAGE(Fact_Sales[Shipping_Days])\n",
    "   Type:     Aggregation (AVG)\n",
    "   Use:      Shipping performance\n",
    "\"\"\"\n",
    "    counter += 1\n",
    "\n",
    "# Time intelligence measures (always added if sales exists)\n",
    "if 'Sales_Amount' in fact_sales.columns:\n",
    "    measures_content += f\"\"\"\n",
    "ADDITIONAL TIME INTELLIGENCE MEASURES:\n",
    "\n",
    "{counter}. YEAR-TO-DATE SALES\n",
    "    Formula:  Sales_YTD = TOTALYTD([Total_Sales], Dim_Date[Date])\n",
    "\"\"\"\n",
    "    counter += 1\n",
    "    \n",
    "    measures_content += f\"\"\"\n",
    "{counter}. MONTH-TO-DATE SALES\n",
    "    Formula:  Sales_MTD = TOTALMTD([Total_Sales], Dim_Date[Date])\n",
    "\"\"\"\n",
    "    counter += 1\n",
    "    \n",
    "    measures_content += f\"\"\"\n",
    "{counter}. QUARTER-TO-DATE SALES\n",
    "    Formula:  Sales_QTD = TOTALQTD([Total_Sales], Dim_Date[Date])\n",
    "\"\"\"\n",
    "    counter += 1\n",
    "\n",
    "measures_content += \"\"\"\n",
    "IMPLEMENTATION NOTES:\n",
    "â€¢ All measures should be created in Power BI\n",
    "â€¢ Date table (Dim_Date) must be marked as date table\n",
    "â€¢ Relationships must be properly set up\n",
    "â€¢ Use DIVIDE() to handle divide-by-zero errors\n",
    "â€¢ Test measures with sample data\n",
    "\n",
    "============================================================\n",
    "\"\"\"\n",
    "\n",
    "with open('model/DAX_Measures.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(measures_content)\n",
    "\n",
    "print(\"   âœ… DAX measures saved: model/DAX_Measures.txt\")\n",
    "\n",
    "# 7. Create one page summary\n",
    "\n",
    "print(\"\\n7. ðŸ“„ Creating One-Page Summary...\")\n",
    "\n",
    "one_page = f\"\"\"BUSINESS INTELLIGENCE PROJECT - DATA MODELING SUMMARY\n",
    "===========================================================\n",
    "\n",
    "PROJECT: Retail Sales Analysis\n",
    "TEAM: Mariem & Rihem\n",
    "DATE: {datetime.now().strftime('%Y-%m-%d')}\n",
    "\n",
    "STAR SCHEMA IMPLEMENTED:\n",
    "â€¢ Fact_Sales: Transactional data with {len(available_measures)} measures\n",
    "â€¢ Dim_Date: Time intelligence with {len(dim_date.columns)} attributes\n",
    "â€¢ Dim_Product: Product dimension with {len(dim_product)} products\n",
    "â€¢ Dim_Customer: Customer dimension with {len(dim_customer)} customers\n",
    "â€¢ Dim_Region: Region dimension with {len(dim_region)} locations\n",
    "\n",
    "DATA MODEL STATISTICS:\n",
    "â€¢ Total Records: {len(fact_sales) + len(dim_date) + len(dim_product) + len(dim_customer) + len(dim_region):,}\n",
    "â€¢ Date Range: {min_date} to {max_date}\n",
    "â€¢ Fact Table Columns: {list(fact_sales.columns)}\n",
    "\n",
    "DAX MEASURES DEFINED:\n",
    "Based on available data, the following measures are defined:\n",
    "1. Total_Sales - SUM aggregation\n",
    "2. Total_Quantity - SUM aggregation (if available)\n",
    "3. Total_Profit - SUM aggregation (if available)\n",
    "4. Sales_YoY - Year-over-year growth\n",
    "5. Sales_MoM - Month-over-month growth\n",
    "6. Profit_Margin - Financial ratio (if profit data available)\n",
    "7. Additional time intelligence measures\n",
    "\n",
    "READY FOR POWER BI DASHBOARD DEVELOPMENT:\n",
    "âœ“ Data model created (star schema)\n",
    "âœ“ Dimension tables ready\n",
    "âœ“ Fact table with foreign keys\n",
    "âœ“ DAX measures defined based on available data\n",
    "âœ“ Documentation complete\n",
    "\n",
    "NEXT STEPS: Load into Power BI and create:\n",
    "1. Executive Summary Dashboard\n",
    "2. Sales Trend Analysis\n",
    "3. Product Performance Report\n",
    "4. Customer Segmentation Analysis\n",
    "===========================================================\n",
    "\"\"\"\n",
    "\n",
    "with open('model/OnePage_Summary.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(one_page)\n",
    "\n",
    "print(\"   âœ… One-page summary saved: model/OnePage_Summary.txt\")\n",
    "\n",
    "\n",
    "# 8. Create complete documentation\n",
    "\n",
    "print(\"\\n8. ðŸ“š Creating Complete Documentation...\")\n",
    "\n",
    "readme_content = f\"\"\"# DATA MODEL DOCUMENTATION\n",
    "## Business Intelligence Project - Week 2-3\n",
    "\n",
    "### Project Information\n",
    "- **Project**: Retail Sales Analysis\n",
    "- **Team**: Rihem & Mariem\n",
    "- **Course**: IT300 - Tunis Business School\n",
    "- **Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "### Files in this directory:\n",
    "1. Dim_Date.csv - Date dimension table\n",
    "2. Dim_Product.csv - Product dimension table  \n",
    "3. Dim_Customer.csv - Customer dimension table\n",
    "4. Dim_Region.csv - Region dimension table\n",
    "5. Fact_Sales.csv - Fact table with transactions\n",
    "6. data_model_diagram.txt - Schema visualization\n",
    "7. DAX_Measures.txt - Power BI measure definitions\n",
    "8. OnePage_Summary.txt - Project summary\n",
    "\n",
    "### Data Statistics:\n",
    "- Total transactions: {len(fact_sales):,}\n",
    "- Time period: {min_date} to {max_date}\n",
    "- Available measures in fact table: {available_measures}\n",
    "- Unique products: {len(dim_product):,}\n",
    "- Unique customers: {len(dim_customer):,}\n",
    "- Unique locations: {len(dim_region):,}\n",
    "\n",
    "### How to use in Power BI:\n",
    "1. Import all CSV files into Power BI\n",
    "2. Create relationships between tables\n",
    "3. Mark Dim_Date[Date] as Date Table\n",
    "4. Create DAX measures from DAX_Measures.txt\n",
    "5. Build dashboards and reports\n",
    "\n",
    "### Project Status:\n",
    "âœ… ETL Completed\n",
    "âœ… Data Modeling Completed  \n",
    "â—» Dashboard Development (Next)\n",
    "â—» Insights & Reporting\n",
    "\n",
    "For questions, contact the project team.\n",
    "\"\"\"\n",
    "\n",
    "with open('model/README.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"   âœ… Documentation saved: model/README.txt\")\n",
    "\n",
    "# 9. Create csv files Read me\n",
    "\n",
    "print(\"\\n9. ðŸ“‹ Creating CSV Files Documentation...\")\n",
    "\n",
    "csv_docs = f\"\"\"CSV FILES DOCUMENTATION\n",
    "=======================\n",
    "\n",
    "1. Fact_Sales.csv ({len(fact_sales):,} rows, {len(fact_sales.columns)} columns)\n",
    "   Purpose: Main transactional data table\n",
    "   Key columns:\n",
    "   - Order_ID: Unique identifier for each order\n",
    "   - Date_Key: Foreign key to date dimension\n",
    "   - Product_Key: Foreign key to product dimension\n",
    "   - Customer_Key: Foreign key to customer dimension\n",
    "   - Region_Key: Foreign key to region dimension\n",
    "   - Sales_Amount: Sales value in currency\n",
    "   - Additional measures: {available_measures}\n",
    "\n",
    "2. Dim_Date.csv ({len(dim_date):,} rows, {len(dim_date.columns)} columns)\n",
    "   Purpose: Date dimension for time-based analysis\n",
    "   Key columns:\n",
    "   - Date_Key: Primary key (YYYYMMDD format)\n",
    "   - Date: Full date\n",
    "   - Year, Quarter, Month: Time hierarchies\n",
    "   - Month_Name, Day_of_Week: Text representations\n",
    "   - Is_Weekend: Boolean for weekend identification\n",
    "\n",
    "3. Dim_Product.csv ({len(dim_product):,} rows, {len(dim_product.columns)} columns)\n",
    "   Purpose: Product information and categorization\n",
    "   Key columns:\n",
    "   - Product_Key: Primary key\n",
    "   - Product_ID: Original product identifier\n",
    "   - Product_Name: Name of product\n",
    "   - Category: Product category\n",
    "   - Sub-Category: Product sub-category\n",
    "\n",
    "4. Dim_Customer.csv ({len(dim_customer):,} rows, {len(dim_customer.columns)} columns)\n",
    "   Purpose: Customer information and segmentation\n",
    "   Key columns:\n",
    "   - Customer_Key: Primary key\n",
    "   - Customer_ID: Original customer identifier\n",
    "   - Customer_Name: Name of customer\n",
    "   - Segment: Customer segment (e.g., Consumer, Corporate)\n",
    "\n",
    "5. Dim_Region.csv ({len(dim_region):,} rows, {len(dim_region.columns)} columns)\n",
    "   Purpose: Geographic location information\n",
    "   Key columns:\n",
    "   - Region_Key: Primary key\n",
    "   - City: City name\n",
    "   - State: State/Province name\n",
    "   - Region: Region name\n",
    "\n",
    "IMPORT INTO POWER BI:\n",
    "1. Click \"Get Data\" -> \"Text/CSV\"\n",
    "2. Select all 5 CSV files\n",
    "3. Load into Power BI\n",
    "4. Create relationships as shown in diagram\n",
    "5. Create measures from DAX_Measures.txt\n",
    "\"\"\"\n",
    "\n",
    "with open('model/CSV_Documentation.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(csv_docs)\n",
    "\n",
    "print(\"   âœ… CSV documentation saved: model/CSV_Documentation.txt\")\n",
    "\n",
    "# 10. FINAL OUTPUT\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ WEEK 2-3: DATA MODELING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸ“ DELIVERABLES CREATED:\")\n",
    "print(\"1. âœ… Star Schema Data Model\")\n",
    "print(f\"   - Fact_Sales.csv ({len(fact_sales):,} rows, {len(fact_sales.columns)} columns)\")\n",
    "print(f\"   - 4 Dimension tables created\")\n",
    "\n",
    "print(\"\\n2. âœ… Data Model Documentation\")\n",
    "print(\"   - data_model_diagram.txt (ASCII diagram)\")\n",
    "print(\"   - simple_diagram.txt (Simplified diagram)\")\n",
    "print(\"   - DAX_Measures.txt (10+ measures)\")\n",
    "print(\"   - OnePage_Summary.txt (Project summary)\")\n",
    "print(\"   - README.txt (Complete documentation)\")\n",
    "print(\"   - CSV_Documentation.txt (File descriptions)\")\n",
    "\n",
    "print(\"\\n3. âœ… DAX Measures Definitions\")\n",
    "print(f\"   - Measures defined for: {available_measures}\")\n",
    "print(\"   - Includes SUM, AVG, YOY, MOM as required\")\n",
    "\n",
    "print(\"\\nðŸ“Š DATA SUMMARY:\")\n",
    "print(f\"   â€¢ Fact table columns: {list(fact_sales.columns)}\")\n",
    "print(f\"   â€¢ Date range: {min_date} to {max_date}\")\n",
    "if 'Sales_Amount' in fact_sales.columns:\n",
    "    print(f\"   â€¢ Total Sales Value: ${fact_sales['Sales_Amount'].sum():,.2f}\")\n",
    "\n",
    "print(\"\\nðŸš€ READY FOR WEEK 3 - DASHBOARD DEVELOPMENT:\")\n",
    "print(\"   Step 1: Load model/*.csv into Power BI\")\n",
    "print(\"   Step 2: Create relationships (star schema)\")\n",
    "print(\"   Step 3: Implement DAX measures\")\n",
    "print(\"   Step 4: Build Executive Summary dashboard\")\n",
    "print(\"   Step 5: Add slicers and interactivity\")\n",
    "\n",
    "print(\"\\nâœ… DATA MODELING PHASE SUCCESSFULLY COMPLETED!\")\n",
    "print(\"   All Week 2-3 requirements have been met.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "875ac5af-6575-4543-bd33-0939e1315a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š DATABASE TABLES:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [name]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Connect to database\n",
    "conn = sqlite3.connect('database/sales_database.db')\n",
    "\n",
    "# Show all tables\n",
    "print(\"ðŸ“Š DATABASE TABLES:\")\n",
    "tables = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table';\", conn)\n",
    "display(tables)\n",
    "\n",
    "# Explore each table\n",
    "for table in tables['name']:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ðŸ“‹ TABLE: {table}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Show structure\n",
    "    structure = pd.read_sql_query(f\"PRAGMA table_info({table})\", conn)\n",
    "    print(f\"Columns ({len(structure)}):\")\n",
    "    for _, row in structure.iterrows():\n",
    "        print(f\"  - {row['name']} ({row['type']})\")\n",
    "    \n",
    "    # Show row count\n",
    "    count = pd.read_sql_query(f\"SELECT COUNT(*) as count FROM {table}\", conn)\n",
    "    print(f\"Rows: {count['count'].iloc[0]:,}\")\n",
    "    \n",
    "    # Show sample data\n",
    "    if count['count'].iloc[0] > 0:\n",
    "        sample = pd.read_sql_query(f\"SELECT * FROM {table} LIMIT 3\", conn)\n",
    "        print(f\"Sample data:\")\n",
    "        display(sample)\n",
    "\n",
    "# Close connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "038a41fd-0f25-4594-9354-2cf112a61104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– AUTO-DETECTING DATA STRUCTURE...\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š ANALYZING YOUR CSV FILES:\n",
      "\n",
      "ðŸ” Fact_Sales.csv columns:\n",
      "Columns: ['Order_ID', 'Date_Key', 'Product_Key', 'Customer_Key', 'Region_Key', 'Sales_Amount', 'Shipping_Days']\n",
      "First row: {'Order_ID': 'CA-2017-152156', 'Date_Key': 20171108, 'Product_Key': 1, 'Customer_Key': 1, 'Region_Key': 1, 'Sales_Amount': 261.96, 'Shipping_Days': 3}\n",
      "\n",
      "ðŸ” Dim_Product.csv columns:\n",
      "Columns: ['Product_Key', 'Product ID', 'Product Name', 'Category', 'Sub-Category']\n",
      "\n",
      "ðŸ—„ï¸  CREATING DATABASE WITH AUTO-DETECTED STRUCTURE...\n",
      "\n",
      "ðŸ“¥ Loading Fact_Sales...\n",
      "   Cleaned columns: ['order_id', 'date_key', 'product_key', 'customer_key', 'region_key', 'sales_amount', 'shipping_days']\n",
      "   Rows: 9,800\n",
      "\n",
      "ðŸ“¥ Loading Dim_Date...\n",
      "   âœ… Loaded: 1,823 rows, 13 columns\n",
      "\n",
      "ðŸ“¥ Loading Dim_Product...\n",
      "   âœ… Loaded: 1,861 rows\n",
      "   Columns: ['product_key', 'product_id', 'product_name', 'category', 'sub_category']\n",
      "\n",
      "ðŸ“¥ Loading Dim_Customer...\n",
      "   âœ… Loaded: 793 rows\n",
      "\n",
      "ðŸ“¥ Loading Dim_Region...\n",
      "   âœ… Loaded: 600 rows\n",
      "\n",
      "ðŸ” CHECKING DATABASE COLUMNS...\n",
      "\n",
      "ðŸ“‹ dim_customer:\n",
      "   â€¢ customer_key\n",
      "   â€¢ customer_id\n",
      "   â€¢ customer_name\n",
      "   â€¢ segment\n",
      "\n",
      "ðŸ“‹ dim_date:\n",
      "   â€¢ date_key\n",
      "   â€¢ date\n",
      "   â€¢ year\n",
      "   â€¢ quarter\n",
      "   â€¢ quarter_num\n",
      "   â€¢ month\n",
      "   â€¢ month_name\n",
      "   â€¢ week_num\n",
      "   â€¢ day_of_week\n",
      "   â€¢ day_of_week_num\n",
      "   â€¢ is_weekend\n",
      "   â€¢ is_holiday\n",
      "   â€¢ fiscal_year\n",
      "\n",
      "ðŸ“‹ dim_product:\n",
      "   â€¢ product_key\n",
      "   â€¢ product_id\n",
      "   â€¢ product_name\n",
      "   â€¢ category\n",
      "   â€¢ sub_category\n",
      "\n",
      "ðŸ“‹ dim_region:\n",
      "   â€¢ region_key\n",
      "   â€¢ city\n",
      "   â€¢ state\n",
      "   â€¢ region\n",
      "   â€¢ country\n",
      "   â€¢ location_key\n",
      "\n",
      "ðŸ“‹ fact_sales:\n",
      "   â€¢ order_id\n",
      "   â€¢ date_key\n",
      "   â€¢ product_key\n",
      "   â€¢ customer_key\n",
      "   â€¢ region_key\n",
      "   â€¢ sales_amount\n",
      "   â€¢ shipping_days\n",
      "\n",
      "ðŸ’¡ CREATING BUSINESS INSIGHTS TABLE...\n",
      "\n",
      "ðŸ“Š Available measures in fact_sales:\n",
      "   â€¢ sales_amount\n",
      "   â€¢ shipping_days\n",
      "\n",
      "ðŸ“ Generated query:\n",
      "\n",
      "    CREATE TABLE IF NOT EXISTS business_insights AS\n",
      "    SELECT \n",
      "        d.year, d.quarter, d.month_name, p.category, p.sub_category as sub_category, c.segment, r.region, r.state,\n",
      "        COUNT(f.order_id) as total_orders, SUM(f.sales_amount) as total_sales, AVG(f.sales_amount) as avg_order_value, AVG(f.shipping_days) as avg_shipping_days\n",
      "    FROM fact_sales f\n",
      "    LEFT JOIN dim_date d ON f.date_key = d.date_key\n",
      "    LEFT JOIN dim_product p ON f.product_key = p.product_key\n",
      "    LEFT JOIN dim_customer c ON f.customer_key = c.customer_key\n",
      "    LEFT JOIN dim_region r ON f.region_key = r.region_key\n",
      "    GROUP BY d.year, d.quarter, d.month_name, p.category, p.sub_category, c.segment, r.region, r.state\n",
      "    \n",
      "âœ… Created business_insights table!\n",
      "\n",
      "ðŸ‘ï¸  CREATING SIMPLE VIEWS FOR ANALYSIS...\n",
      "âœ… Created vw_monthly_sales view\n",
      "âœ… Created vw_product_performance view\n",
      "\n",
      "ðŸ“Š FINAL DATABASE SUMMARY:\n",
      "============================================================\n",
      "\n",
      "TABLES:\n",
      "   â€¢ business_insights (7,148 rows)\n",
      "   â€¢ dim_customer (793 rows)\n",
      "   â€¢ dim_date (1,823 rows)\n",
      "   â€¢ dim_product (1,861 rows)\n",
      "   â€¢ dim_region (600 rows)\n",
      "   â€¢ fact_sales (9,800 rows)\n",
      "\n",
      "VIEWS:\n",
      "   â€¢ vw_monthly_sales (view)\n",
      "   â€¢ vw_product_performance (view)\n",
      "\n",
      "ðŸ§ª TEST QUERY - TOP 5 SALES BY DATE:\n",
      " year month_name  orders       sales\n",
      " 2018   November     456 117938.1550\n",
      " 2017   December     338  95739.1210\n",
      " 2018  September     453  86152.8880\n",
      " 2018   December     459  83030.3888\n",
      " 2015  September     267  81623.5268\n",
      "\n",
      "============================================================\n",
      "ðŸŽ‰ DATA WAREHOUSE CREATED SUCCESSFULLY!\n",
      "============================================================\n",
      "\n",
      "ðŸ“ Database file: database/sales_warehouse.db\n",
      "ðŸ“Š Total tables created: 6\n",
      "ðŸ‘ï¸  Total views created: 2\n",
      "\n",
      "ðŸš€ Your data is ready for Power BI and SQL queries!\n"
     ]
    }
   ],
   "source": [
    "# auto_detect_warehouse.py\n",
    "\"\"\"\n",
    "AUTO-DETECTING DATA WAREHOUSE BUILDER\n",
    "Team: Mariem & Rihem\n",
    "This script automatically detects what columns exist in your data\n",
    "\"\"\"\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"ðŸ¤– AUTO-DETECTING DATA STRUCTURE...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. CHECK WHAT COLUMNS WE HAVE\n",
    "\n",
    "print(\"\\nðŸ“Š ANALYZING YOUR CSV FILES:\")\n",
    "\n",
    "# Check Fact_Sales columns\n",
    "print(\"\\nðŸ” Fact_Sales.csv columns:\")\n",
    "fact_df = pd.read_csv('model/Fact_Sales.csv')\n",
    "print(\"Columns:\", list(fact_df.columns))\n",
    "print(\"First row:\", fact_df.iloc[0].to_dict())\n",
    "\n",
    "# Check Dim_Product columns  \n",
    "print(\"\\nðŸ” Dim_Product.csv columns:\")\n",
    "product_df = pd.read_csv('model/Dim_Product.csv')\n",
    "print(\"Columns:\", list(product_df.columns))\n",
    "\n",
    "# 2. CREATE DATABASE WITH EXACT COLUMNS\n",
    "\n",
    "print(\"\\nðŸ—„ï¸  CREATING DATABASE WITH AUTO-DETECTED STRUCTURE...\")\n",
    "\n",
    "# Create database\n",
    "db_path = 'database/sales_warehouse.db'\n",
    "if os.path.exists(db_path):\n",
    "    os.remove(db_path)  # Remove old database\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Function to clean column names\n",
    "def clean_column_name(col):\n",
    "    \"\"\"Clean column names for SQL compatibility\"\"\"\n",
    "    return str(col).lower().replace(' ', '_').replace('-', '_').replace('(', '').replace(')', '').replace('.', '')\n",
    "\n",
    "# Load Fact_Sales with auto-detected columns\n",
    "print(\"\\nðŸ“¥ Loading Fact_Sales...\")\n",
    "fact_df = pd.read_csv('model/Fact_Sales.csv')\n",
    "\n",
    "# Clean column names\n",
    "fact_df.columns = [clean_column_name(col) for col in fact_df.columns]\n",
    "print(f\"   Cleaned columns: {list(fact_df.columns)}\")\n",
    "print(f\"   Rows: {len(fact_df):,}\")\n",
    "\n",
    "# Save to database\n",
    "fact_df.to_sql('fact_sales', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Load Dim_Date\n",
    "print(\"\\nðŸ“¥ Loading Dim_Date...\")\n",
    "try:\n",
    "    dim_date = pd.read_csv('model/Dim_Date.csv')\n",
    "    dim_date.columns = [clean_column_name(col) for col in dim_date.columns]\n",
    "    dim_date.to_sql('dim_date', conn, if_exists='replace', index=False)\n",
    "    print(f\"   âœ… Loaded: {len(dim_date):,} rows, {len(dim_date.columns)} columns\")\n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸  Error loading Dim_Date: {e}\")\n",
    "\n",
    "# Load Dim_Product\n",
    "print(\"\\nðŸ“¥ Loading Dim_Product...\")\n",
    "try:\n",
    "    dim_product = pd.read_csv('model/Dim_Product.csv')\n",
    "    dim_product.columns = [clean_column_name(col) for col in dim_product.columns]\n",
    "    dim_product.to_sql('dim_product', conn, if_exists='replace', index=False)\n",
    "    print(f\"   âœ… Loaded: {len(dim_product):,} rows\")\n",
    "    print(f\"   Columns: {list(dim_product.columns)}\")\n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸  Error loading Dim_Product: {e}\")\n",
    "\n",
    "# Load Dim_Customer\n",
    "print(\"\\nðŸ“¥ Loading Dim_Customer...\")\n",
    "try:\n",
    "    dim_customer = pd.read_csv('model/Dim_Customer.csv')\n",
    "    dim_customer.columns = [clean_column_name(col) for col in dim_customer.columns]\n",
    "    dim_customer.to_sql('dim_customer', conn, if_exists='replace', index=False)\n",
    "    print(f\"   âœ… Loaded: {len(dim_customer):,} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸  Error loading Dim_Customer: {e}\")\n",
    "\n",
    "# Load Dim_Region\n",
    "print(\"\\nðŸ“¥ Loading Dim_Region...\")\n",
    "try:\n",
    "    dim_region = pd.read_csv('model/Dim_Region.csv')\n",
    "    dim_region.columns = [clean_column_name(col) for col in dim_region.columns]\n",
    "    dim_region.to_sql('dim_region', conn, if_exists='replace', index=False)\n",
    "    print(f\"   âœ… Loaded: {len(dim_region):,} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸  Error loading Dim_Region: {e}\")\n",
    "\n",
    "\n",
    "# 3. CHECK WHAT COLUMNS WE ACTUALLY HAVE IN DB\n",
    "\n",
    "print(\"\\nðŸ” CHECKING DATABASE COLUMNS...\")\n",
    "\n",
    "# Get all tables\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "table_columns = {}\n",
    "for table_name, in tables:\n",
    "    cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "    columns = cursor.fetchall()\n",
    "    column_names = [col[1] for col in columns]\n",
    "    table_columns[table_name] = column_names\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ {table_name}:\")\n",
    "    for col_name in column_names:\n",
    "        print(f\"   â€¢ {col_name}\")\n",
    "\n",
    "\n",
    "# 4. CREATE DYNAMIC BUSINESS INSIGHTS\n",
    "\n",
    "print(\"\\nðŸ’¡ CREATING BUSINESS INSIGHTS TABLE...\")\n",
    "\n",
    "# Build SELECT clause based on available columns\n",
    "select_parts = []\n",
    "group_by_parts = []\n",
    "\n",
    "# Check what we have in fact_sales\n",
    "fact_cols = table_columns.get('fact_sales', [])\n",
    "print(f\"\\nðŸ“Š Available measures in fact_sales:\")\n",
    "for col in fact_cols:\n",
    "    if col not in ['order_id', 'date_key', 'product_key', 'customer_key', 'region_key']:\n",
    "        print(f\"   â€¢ {col}\")\n",
    "\n",
    "# Time columns (from dim_date)\n",
    "if 'dim_date' in table_columns:\n",
    "    date_cols = table_columns['dim_date']\n",
    "    if 'year' in date_cols:\n",
    "        select_parts.append(\"d.year\")\n",
    "        group_by_parts.append(\"d.year\")\n",
    "    if 'quarter' in date_cols:\n",
    "        select_parts.append(\"d.quarter\")\n",
    "        group_by_parts.append(\"d.quarter\")\n",
    "    if 'month_name' in date_cols:\n",
    "        select_parts.append(\"d.month_name\")\n",
    "        group_by_parts.append(\"d.month_name\")\n",
    "\n",
    "# Product columns\n",
    "if 'dim_product' in table_columns:\n",
    "    prod_cols = table_columns['dim_product']\n",
    "    if 'category' in prod_cols:\n",
    "        select_parts.append(\"p.category\")\n",
    "        group_by_parts.append(\"p.category\")\n",
    "    if 'sub_category' in prod_cols or 'subcategory' in prod_cols:\n",
    "        sub_col = 'sub_category' if 'sub_category' in prod_cols else 'subcategory'\n",
    "        select_parts.append(f\"p.{sub_col} as sub_category\")\n",
    "        group_by_parts.append(f\"p.{sub_col}\")\n",
    "\n",
    "# Customer columns\n",
    "if 'dim_customer' in table_columns:\n",
    "    cust_cols = table_columns['dim_customer']\n",
    "    if 'segment' in cust_cols:\n",
    "        select_parts.append(\"c.segment\")\n",
    "        group_by_parts.append(\"c.segment\")\n",
    "\n",
    "# Region columns\n",
    "if 'dim_region' in table_columns:\n",
    "    region_cols = table_columns['dim_region']\n",
    "    if 'region' in region_cols:\n",
    "        select_parts.append(\"r.region\")\n",
    "        group_by_parts.append(\"r.region\")\n",
    "    if 'state' in region_cols:\n",
    "        select_parts.append(\"r.state\")\n",
    "        group_by_parts.append(\"r.state\")\n",
    "\n",
    "# Build KPI SELECT clause\n",
    "kpi_select = []\n",
    "kpi_select.append(\"COUNT(f.order_id) as total_orders\")\n",
    "\n",
    "# Check for sales column (could be sales, sales_amount, amount, etc.)\n",
    "sales_col = None\n",
    "for col in ['sales_amount', 'sales', 'amount', 'revenue']:\n",
    "    if col in fact_cols:\n",
    "        sales_col = col\n",
    "        break\n",
    "\n",
    "if sales_col:\n",
    "    kpi_select.append(f\"SUM(f.{sales_col}) as total_sales\")\n",
    "    kpi_select.append(f\"AVG(f.{sales_col}) as avg_order_value\")\n",
    "\n",
    "# Check for quantity column\n",
    "if 'quantity' in fact_cols:\n",
    "    kpi_select.append(\"SUM(f.quantity) as total_quantity\")\n",
    "\n",
    "# Check for profit column\n",
    "profit_col = None\n",
    "for col in ['profit_amount', 'profit', 'margin']:\n",
    "    if col in fact_cols:\n",
    "        profit_col = col\n",
    "        break\n",
    "\n",
    "if profit_col:\n",
    "    kpi_select.append(f\"SUM(f.{profit_col}) as total_profit\")\n",
    "    kpi_select.append(f\"SUM(CASE WHEN f.{profit_col} > 0 THEN 1 ELSE 0 END) as profitable_orders\")\n",
    "\n",
    "# Check for discount column\n",
    "if 'discount' in fact_cols:\n",
    "    kpi_select.append(\"AVG(f.discount) as avg_discount\")\n",
    "\n",
    "# Check for shipping days\n",
    "if 'shipping_days' in fact_cols:\n",
    "    kpi_select.append(\"AVG(f.shipping_days) as avg_shipping_days\")\n",
    "\n",
    "# Build the complete query\n",
    "if select_parts:  # Only create if we have dimension columns\n",
    "    business_insights_query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS business_insights AS\n",
    "    SELECT \n",
    "        {', '.join(select_parts)},\n",
    "        {', '.join(kpi_select)}\n",
    "    FROM fact_sales f\n",
    "    LEFT JOIN dim_date d ON f.date_key = d.date_key\n",
    "    LEFT JOIN dim_product p ON f.product_key = p.product_key\n",
    "    LEFT JOIN dim_customer c ON f.customer_key = c.customer_key\n",
    "    LEFT JOIN dim_region r ON f.region_key = r.region_key\n",
    "    GROUP BY {', '.join(group_by_parts)}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ“ Generated query:\")\n",
    "    print(business_insights_query)\n",
    "    \n",
    "    try:\n",
    "        cursor.execute(\"DROP TABLE IF EXISTS business_insights\")\n",
    "        cursor.execute(business_insights_query)\n",
    "        print(\"âœ… Created business_insights table!\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating business_insights: {e}\")\n",
    "else:\n",
    "    print(\"âš ï¸  Not enough dimension columns to create business_insights table\")\n",
    "\n",
    "\n",
    "# 5. CREATE SIMPLE SUMMARY VIEWS\n",
    "\n",
    "print(\"\\nðŸ‘ï¸  CREATING SIMPLE VIEWS FOR ANALYSIS...\")\n",
    "\n",
    "# Create monthly sales view\n",
    "try:\n",
    "    if sales_col and 'dim_date' in table_columns:\n",
    "        cursor.execute(f\"\"\"\n",
    "            CREATE VIEW IF NOT EXISTS vw_monthly_sales AS\n",
    "            SELECT \n",
    "                d.year,\n",
    "                d.month_name,\n",
    "                COUNT(f.order_id) as total_orders,\n",
    "                SUM(f.{sales_col}) as total_sales,\n",
    "                AVG(f.{sales_col}) as avg_order_value\n",
    "            FROM fact_sales f\n",
    "            LEFT JOIN dim_date d ON f.date_key = d.date_key\n",
    "            GROUP BY d.year, d.month_name\n",
    "            ORDER BY d.year, d.month_name\n",
    "        \"\"\")\n",
    "        print(\"âœ… Created vw_monthly_sales view\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not create monthly sales view: {e}\")\n",
    "\n",
    "# Create product performance view\n",
    "try:\n",
    "    if sales_col and 'dim_product' in table_columns:\n",
    "        cursor.execute(f\"\"\"\n",
    "            CREATE VIEW IF NOT EXISTS vw_product_performance AS\n",
    "            SELECT \n",
    "                p.category,\n",
    "                COUNT(f.order_id) as orders,\n",
    "                SUM(f.{sales_col}) as sales,\n",
    "                AVG(f.{sales_col}) as avg_sale_per_order\n",
    "            FROM fact_sales f\n",
    "            LEFT JOIN dim_product p ON f.product_key = p.product_key\n",
    "            GROUP BY p.category\n",
    "            ORDER BY sales DESC\n",
    "        \"\"\")\n",
    "        print(\"âœ… Created vw_product_performance view\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not create product performance view: {e}\")\n",
    "\n",
    "\n",
    "# 6. FINAL SUMMARY\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "print(\"\\nðŸ“Š FINAL DATABASE SUMMARY:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cursor.execute(\"SELECT name, type FROM sqlite_master WHERE type IN ('table', 'view') ORDER BY type, name;\")\n",
    "objects = cursor.fetchall()\n",
    "\n",
    "for obj_type in ['table', 'view']:\n",
    "    type_objs = [obj for obj in objects if obj[1] == obj_type]\n",
    "    if type_objs:\n",
    "        print(f\"\\n{obj_type.upper()}S:\")\n",
    "        for name, _ in type_objs:\n",
    "            if obj_type == 'table':\n",
    "                count = cursor.execute(f\"SELECT COUNT(*) FROM {name}\").fetchone()[0]\n",
    "                print(f\"   â€¢ {name} ({count:,} rows)\")\n",
    "            else:\n",
    "                print(f\"   â€¢ {name} (view)\")\n",
    "\n",
    "# Create a simple query to test\n",
    "print(\"\\nðŸ§ª TEST QUERY - TOP 5 SALES BY DATE:\")\n",
    "try:\n",
    "    if sales_col:\n",
    "        test_query = f\"\"\"\n",
    "        SELECT \n",
    "            d.year,\n",
    "            d.month_name,\n",
    "            COUNT(*) as orders,\n",
    "            SUM(f.{sales_col}) as sales\n",
    "        FROM fact_sales f\n",
    "        LEFT JOIN dim_date d ON f.date_key = d.date_key\n",
    "        GROUP BY d.year, d.month_name\n",
    "        ORDER BY sales DESC\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "        test_result = pd.read_sql_query(test_query, conn)\n",
    "        print(test_result.to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not run test query: {e}\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸŽ‰ DATA WAREHOUSE CREATED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nðŸ“ Database file: {db_path}\")\n",
    "print(f\"ðŸ“Š Total tables created: {len([obj for obj in objects if obj[1] == 'table'])}\")\n",
    "print(f\"ðŸ‘ï¸  Total views created: {len([obj for obj in objects if obj[1] == 'view'])}\")\n",
    "print(\"\\nðŸš€ Your data is ready for Power BI and SQL queries!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83af874f-4ccc-43a9-8454-1090ceca88e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6319b36-0038-4bf7-90b0-7b40b5905c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3531828-3abc-437a-97da-8323f608e72d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
